% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related Work}%
\label{chap:related}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:relwork}
Unlike in chapter~\ref{chap:background}, here we introduce some of the less widely known models and approaches that address the same tasks we are trying to solve.
The purpose of this section is to put our work in context, offer a comparison, and provide a broader intuition about what our ideas are based on.

\section{Modular dialogue architectures}
\todo{more recent works?}
\label{relwork:modular}
The traditional dialogue system implementation, especially for task-oriented dialogues, is based on modular architecture.
The modular system consists of several components connected to form a pipeline.
First, the Natural Language Understanding (NLU) module parses the utterance and creates a structured representation.
Based on NLU outputs, the dialogue management module determines the next action.
Dialogue Management usually consists of the state tracker that updates the state based on NLU outputs and the policy module that chooses the action.
Finally, the language generation module is used to verbalize the chosen action.
\subsection{Natural Language Understanding (NLU)}
From the machine learning point of view, the intent and domain detection can be seen as a classification task, and sentence-level classification can be utilized \cite{yaman2008integrative,schapire2000boostexter}.
The slot-value filling can be approached as a sequence tagging problem.
Many approaches have been proposed to tackle this issue, ranging from SVM \cite{shi2016recurrent} and HMM \cite{surendran2006dialog} based taggers to various neural models \cite{adel2016comparing, zhang2017position, mesnil2014using}.
It is reasonable to model them jointly because of the similar nature of these three sub-tasks.
Modeling the intent detection together with slot filling proved to be beneficial for the model performance \cite{zhang2017position, liu2016attention, xu2013convolutional}.

\subsection{Dialogue State Tracking (DST)} 
\label{relwork:DST}
The most straightforward solution to this problem is a rule-based system that tracks the current slot values based on NLU.
However, the situation is usually more complicated.
We must consider a distribution of slot value probabilities, and the update rules can be rather complex.
\citet{vzilka2013comparison} compares different data-driven models for dialogue state tracking.
Neural networks have also been used to model the distributions \cite{mrkvsic2016neural, zhong2018global} and deal with multiple domain handling \cite{rastogi2017scalable}.

\subsection{Dialogue Policy}
\label{relwork:policy}
The policy decision can thus be framed as a classification task \cite{gavsic2013gaussian}.
Learning the policy just from the offline data might not produce robust policy due to low variability in the data.
Therefore, many works model the dialogue as a partially observable Markov decision process \cite{gavsic2010gaussian, thomson2010bayesian}.
Reinforcement learning techniques are then applied to learn the policy and incorporate human feedback \cite{peng2017composite, su2016line}.

\subsection{Natural Language Generation (NLG)}
NLG is often realized with a set of handcrafted templates that are selected heuristically \cite{rudnicky_creating_1999}.
The variability of the generated utterances is limited, and the scalability is poor.
Therefore, corpus-based methods have been proposed \cite{oh2000stochastic, mairesse-young-2014-stochastic}.
Lately, neural network based systems were proposed as well \cite{wen-etal-2015-semantically, wen-etal-2016-multi}

\section{End-to-end architectures}
\label{relwork:end-to-end}
Various end-to-end solutions have been proposed to address the drawbacks of modular system training (discussed in Section~\ref{02:ds-background}).
This was made possible largely thanks to the growing popularity of Neural Networks (NN) and the backpropagation algorithm over the last decade (see Section~\ref{background:nns}.
NN forms a family of models that naturally allow us to combine and train multiple models using a single training algorithm.
Therefore, several solutions were proposed to implement the respective modules using neural network-based models, interconnecting them to form the pipeline and train them jointly \cite{li-etal-2017-end,wen-etal-2017-network}.
Although the end-to-end training improves the scalability of the models, the proposed architectures still require multiple levels of data annotation for training.
To mitigate this problem, \citet{serban2016building} proposed a hierarchical end-to-end model that uses two levels of encoder-decoder Recurrent Neural Networks (RNN), one operating on dialogue turn level for keeping long-term context and one operating on word level for analyzing the current user input.
It does not follow the traditional pipeline scheme and thus does not require expert annotations.
However, it is unsuitable for practical use in task-oriented DS in its raw form due to its low performance and insufficient robustness.
The idea was further extended by \citet{williams2017hybrid}, who introduced the \textit{Hybrid Code Networks}, an architecture that uses multiple utterance representations customizable by the developer.
Despite good performance and flexibility, the proposed model again required a non-trivial amount of data annotation.
\citet{lei2018sequicity} devised a novel idea to model the dialogue with an extended sequence-to-sequence model.
\label{sec:sequicity}
They use an encoder-decoder architecture based on RNN that generates a dialogue state before response generation.
They summarize the dialogue history in the RNN hidden state and use a system of copy mechanisms to track the dialogue state.
The proposed dialogue state representation is greatly simplified and does not require explicit NLU input.
Thus, the annotation process is significantly easier.

In recent years, the NLP world has witnessed the great success of attention-based models (Transformers) \cite{vaswani2017attention} and their usage as pre-trained language models \cite{devlin-etal-2019-bert}.
In dialogue systems, these models also show prominent results in the open-domain setting \cite{DBLP:journals/corr/abs-1901-08149} or for dialogue state tracking \cite{chao2019bert}.
The pretrained models are naturally utilizable for transfer learning, which proved useful in dialogue domain adaptation task \cite{shalyminov-etal-2019-shot}.
Recently, attention-based architecture was proposed that models latent dialogue actions \cite{bao2019plato}.

Task-oriented dialogue modeling with the use of pretrained language models was researched by \citet{zhang2019dialogpt} or \citet{peng-etal-2021-soloist}, who followed the ideas of text-based state encoding and 2-stage generation proposed in the Sequicity model \cite{lei2018sequicity}.
This approach first uses a text-based model to decode the structured belief state.
The belief state is later used to retrieve db information optionally and finally, the model is called once more, conditioned on the belief state and retrieved information to generate a response.
Several other improvements were then proposed to the architecture that either improve contrastive state training \cite{kulhanek-etal-2021-augpt} or redefine state tracking as generation of belief state differences \cite{lin-etal-2020-mintl}.
Others also proposed combining the purely generative models and retrieval-based approaches\cite{pandey-etal-2018-exemplar,cai-etal-2019-retrieval,nekvinda-dusek-2022-aargh}.
The above-mentioned works finetuned the model on the in-domain data, contrasting with the pure in-context learning approach we apply in Chapter~\ref{chap:llms}.

% \label{relwork:llm-dialogue}
% The Transformer architecture \cite{vaswani2017attention} enabled training larger and more capable language models.
% The research on their few-shot and zero-shot abilities probably dates back to the GPT-2 and GPT-3 models \cite{radford2019language,brown2020language}, which are scaled versions of the Transformer decoder.
% Many followed this path of training large Transformer decoders \cite{zhang2022opt,black2022gpt}, yielding models of sizes up to hundreds of billions parameters big \cite{zhao_survey_2023}.
% There are also models that leverage the whole original Transformer architecture (encoder and decoder) such as T5 \cite{2020t5}.
% Recently, we've seen research focusing on training smaller architectures with similar capabilities and thus making the usage of LLMs available to the general public \cite{touvron2023llama}.

\subsection{Instruction Tuning for Large Language Models}
Here, we follow up on the introduction to Large Language Models (LLMs) in Section~\ref{background:plms}.
In particular, we discuss instruction-tuning techniques that aim to make LLMs more accessible. 
The idea of using reinforcement learning techniques to align model-based agents better with users' intents was pioneered in game agent development. \cite{christiano2017deep} and later explored for training language models \cite{ziegler2019fine,ouyang2022training}.
Although these techniques proved quite effective, the process is still very demanding in collecting user feedback.
Consequently, several datasets were proposed \cite{supernaturalinstructions,iyer2022opt,black2022gpt} that collected millions of instructions-based tasks in natural language and can be applied to align LMs similarly to reinforcement learning.

\section{Unsupervised and transfer learning methods}
\label{sec:relwork-unsup}
The research of methods that reduce the amount of supervision needed can be divided into two paradigms.
One research direction tries to construct a method of unsupervised or weakly supervised data analysis, focusing on a certain part of the dialogue pipeline.
Such a method can provide artificial supervision for the supervised models introduced earlier.
The other option is to design a model that inherently does not need supervision or requires less annotation.

\subsection{Unsupervised analysis and labeling for NLU}
Various methods have been proposed to deal with NLU without explicit supervision.
\citet{chen2016zero} first proposed a model for zero-shot user intent embedding prediction by training a convolutional neural network to score the sentence-intent similarities.
Recently, \citet{shi2018auto} proposed an intent detection model using sentence clustering based on sentence-level features.
They have applied their method successfully for the task of intent detection.

Using semantic relations to perform language understanding in the unsupervised setting was proposed by \citet{heck2012exploiting}.
Here, the authors use the Semantic Web \cite{berners2001semantic}, which is a triple-based database of entity relations.
Their approach relies heavily on structured web pages for the target domain.
They exploit the structure to obtain semantic annotations in an unsupervised setting.

\label{sec:relwork-chen}
\citet{chen2014leveraging} combine the paradigms of semantic frame parsing with distributional semantics to perform unsupervised semantic slot induction.
The authors further improve their model in \citet{chen2015jointly} where they select the most prominent slot candidates using lexical knowledge graphs.
However, both approaches only output a ranking of potential slot candidates based on frames.
Since frame annotation is very fine-grained, this produces a huge number of candidates, requiring their manual merging  %of frame labels 
into slots for any practical use.
In contrast, we determine domain-relevant slots automatically.
\citet{coope_span-convert_2020} focus on a few-shot setting and perform span extraction of slot values using pretrained models in Chapter~\ref{chap:data_analysis}.
Another direction of research focuses on zero-shot slot filling.  \citet{bapna2017towards}'s recurrent-neural-network-based slot tagger is pretrained on multiple domains and takes a textual description of the target slot on the input in addition to the user utterance. This way, adapting to a new domain only involves providing new slot descriptions.
Further works extend this idea with more complex architectures \cite{shah2019robust,liu2020coach}.
An interesting follow-up work was presented in \citet{yu-etal-2022-unsupervised} which discovered dialogue schema slot candidates by analyzing attention spans of pretrained LMs and clustering the spans with the DBSCAN algorithm.
Recently, \citet{qiu2022structure} proposed an alternative way to dialogue slot discovery by using pretrained sequence tagging models based on BERT taggers.

\subsection{Dialogue structure discovery}
\citet{brychcin2016unsupervised} focused on modeling the dialogue as a Markov decision process using HMMs.
By fitting the HMMs to the data, they explore the dialogue dynamics and assign Dialogue Acts to the HMM states.
Later, people based the structure discovery on the VRNN-based models.
In the DVRNN model \cite{shi2019unsupervised}, the authors train an unsupervised VRNN model with discrete latent states and explore transition probabilities of the neighboring latent states to explore the dialogue structure.
This approach is later improved by \citet{qiu-etal-2020-structured}, who propose to augment VRNN with the CRF layer in their SVRNN model.
Yet another work that combines LM-based representations and HMM is presented in \citet{lu-etal-2022-unsupervised}.

\subsection{Modeling dialogue generation with less supervision}
Work regarding using semi-supervised or unsupervised methods for the dialogue response generation task as a whole in the task-oriented setting has been limited so far.
One of the main challenges is to model the dialogue state with no supervision since it is structured and might be quite complex.

The method proposed by \citet{jin2018explicit} builds on \citet{lei2018sequicity}'s sequence-to-sequence dialogue model (see Section~\ref{sec:sequicity}) by introducing a posterior regularization term in the loss function.
The model has two modules, a teacher and a student, to track the dialogue state and works semi-supervised.
For supervised data, both tracker modules are trained with supervised classification loss.
For unsupervised data, the teacher module can look at system responses, therefore, it operates with more input information and makes more accurate predictions.
The student module is then trained to minimize the KL divergence loss.
The teacher module is conditioned on the system response, so it cannot be used when the model is deployed, but it helps to train the student even with unlabeled data.

\citet{wen2017latent} introduced a model that learns latent intentions, bypassing the explicit dialogue state modeling.
\citet{zhao-eskenazi-2018-zero} approached the problem differently.
They designed a novel dialogue system model based on VAEs.
Their model uses supervised data from one domain to learn latent action representations.
Their recognition module is learned to map utterance representations to the same feature space as the action representations.
When transferring to another domain, the model needs only a few seed responses to adapt.
Based on this idea, other works followed \citep{shalyminov-etal-2019-shot, huang2019mala}.

\subsection{Few-shot dialogue modelling}
One of the first neural network based models focusing on learning dialogue from a few in-domain examples was the Hybrid Code Networks  \cite{williams-etal-2017-hybrid}, a trainable system based on recurrent neural networks with partially handcrafted components.
Another approach was proposed in \citet{zhao-eskenazi-2018-zero}, which used latent action representations to transfer domain knowledge.
Latent actions were also used in \citet{huang2020mala} or \citet{shalyminov-etal-2019-data}.
More recent approaches use the Transformer architecture and pretrained language models \cite{shalyminov_fast_2020} to leverage these models' abilities obtained during large-scale pretraining.
Another example is \citet{madotto2020language} or
\citet{hu-etal-2022-context}, which used LLMs and in-context learning to perform belief state tracking.
They did not use instruction-tuned models and formulated the task as an SQL query generation.
However, they omit the response generation as well as database retrieval.