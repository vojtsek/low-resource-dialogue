% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}%
\label{chap:intro}
Human language is a convenient and natural means of communication for human beings.
It is, therefore, desirable to implement an interface that mimics natural language and allows humans to interact with computers like they would with other human individuals.

To achieve this goal, we need to be able to transfer information between human users and the computer.
Humans most often use speech or writing to encode and transfer information, so various techniques have been invented that deal with this kind of encoding, such as Automatic Speech Recognition (ASR), Optical Character Recognition (OCR), and Text-to-speech Synthesis (TTS).
However, to efficiently transfer information, we need the ability to engage in a conversational exchange.
A conversation (dialogue) offers additional means of communication such as clarification, information updates, or more effective encoding through context reference, etc.

To perform meaningful dialogue, we need more than just mimicking the interface.
The computer should understand the process of gradual information exchange and be able to capture the meaning of utterances in the context.
Moreover, the system must provide relevant responses to engage in the conversation successfully\cite{jurafsky2000speech,mctear2022conversational}.

In this work, we focus on this part of the problem, i.e. we do not care about encoding or decoding natural language in a signal such as speech.
Rather, we assume textual interfaces for both input and output.
Put simply, the task of a Dialogue System (DS)\cite{jurafsky2000speech} is to generate the correct natural language response \textit{r} given the natural language user utterance \textit{u} and context \textit{c}.
Importantly, it is not always clear what we mean by a ``correct utterance'' in a particular context.
This can depend on several conditions, requirements, and constraints.
In this work, we consider task-oriented dialogues, which are well-defined in this aspect.
This concept is introduced in more detail in Section~\ref{chap:background}.
We understand the dialogue as a \textit{turn-taking} conversation, i.e. participants (user and system) communicate in alternating \textit{turns}.
In this work, we exclusively focus on two-party dialogues.

Considering the conversation history, the ultimate goal is to construct a dialogue agent that provides meaningful responses to all kinds of questions.
Such an agent would effectively pass the Turing test, the holy grail for Artificial Intelligence~\cite{pinar2000turing}.
The development of Large Language Models (LLMs) and their instruction tuning brings us closer to achieving this goal~\cite{rothman2021transformers}.
Nevertheless, we do not need to achieve such complexity in many real-life cases.
For example, we can consider situated artificial agents which solely focus on achieving a certain well-specified goal, such as ordering food or reserving a flight ticket.

Dialogue systems promise a convenient means of communication between humans and computers.
They allow voice interaction, making it especially well suited for applications that should not disrupt attention, such as car control.
Systems capable of human-like conversation and accomplishing given tasks have huge potential to automate technical support processes and call centers or serve as personal assistants.

Despite some successful dialogue system deployments, current dialogue systems still suffer from several drawbacks.
Usually, the DSs are tailored to specific applications, and applying them in other domains is hard.
Typically, the system is customized to handle a set of predefined domains with a high success rate.
A lot of effort goes into designing an ontology and handling domain-specific scenarios.
Even in the era of large language models, this is still the case for the predominant part of commercial applications.
This results in bad scalability and inflexible usage.
Ideally, a system would learn common behavioral patterns required to finish the defined goal through conversational exchange successfully.
Given some description data, it could apply the learned knowledge to previously unseen domains and applications.
Although the LLMs make a huge step forward in this ability, they still might require finetuning and are not yet suitable for direct applications in the task-oriented world~\cite{iizuka2023clarifying,hudecek-dusek-2023-large} which we also discuss in Chapter~\ref{chap:llms}.

Another problem is that there seems to be a trade-off between interpretability and performance or scalability of the systems in the case of neural network-based models.
In most cases, the more complex and capable the model is, the harder it is to interpret its behavior and explain its decisions.

This thesis proposes solutions to some of these problems, especially in the task-oriented setting.
We now outline the main goals we want to achieve and the ways we address them in our experiments:
\begin{enumerate}
    \item \textbf{Limit the amount of supervision needed}. Extending the system is hard since it requires significant expert effort to design the schemas and annotate the data. In Chapter~\ref{chap:data_analysis}, we propose an automatic data analysis tool to gather information from dialogue corpora and suggest annotation schema without direct supervision.
    \item \textbf{Enable the dialogue systems to leverage large unannotated data sets and train more robust models}. There has not been much work on training task-oriented systems in an unsupervised way. We delve into this problem and suggest using latent variable models in Chapter~\ref{chap:modeling} and explore the usage of pre-trained language models, which leverage large unannotated corpora, in Chapters~\ref{06:chap:lm-tod}, \ref{chap:llms}.
    \item \textbf{Be able to train the systems with less data overall} It is difficult for the current system architectures to transfer the learned knowledge to new domains. To explore this phenomenon more, we conduct experiments to see if language models can transfer the knowledge in Chapter~\ref{06:chap:lm-tod}. We also explore how to train LLMs with a limited number of examples in Chapter~\ref{chap:llms}.
\end{enumerate}

Scalability and domain adaptation go hand in hand.
We focus on reducing the annotation needed to train a system and on knowledge abstraction to make transfer learning possible.
To leverage larger data sets, we explore unsupervised techniques that do not require annotation, making the data collection process substantially easier.

In addition to the experimental chapters mentioned above, this thesis also includes Chapters~\ref{chap:background},\ref{chap:related}, which introduce the theoretical concepts and related from which we take inspiration or use for comparison.
Finally, Chapter~\ref{chap:conclusion} summarizes our findings and proposes future research directions.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


